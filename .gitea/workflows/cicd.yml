name: CI/CD ChronoTrak

on:
  push:
    branches: [ main, master ]
  pull_request:
    branches: [ main, master ]
  workflow_dispatch:

jobs:
  test:
    runs-on: docker
    steps:
      - name: Configuration du runner
        run: |
          # Installer les outils de base (Alpine)
          apk add --no-cache \
          git \
          curl \
          ca-certificates \
          bash \
          nodejs \
          npm \
          python3 \
          py3-pip \
          build-base \
          libffi-dev
          
          # Cr√©er un lien symbolique python -> python3 si n√©cessaire
          if ! command -v python >/dev/null 2>&1; then
            ln -s $(which python3) /usr/local/bin/python || true
          fi
          
          # Installation de uv (gestionnaire de paquets Python ultra-rapide)
          echo "üì¶ Installation de uv..."
          curl -LsSf https://astral.sh/uv/install.sh | sh
          export PATH="$HOME/.local/bin:$PATH"
          
          # V√©rifier la version de Python disponible
          PYTHON_VERSION=$(python3 --version 2>&1 | awk '{print $2}')
          echo "Python disponible: $PYTHON_VERSION"
          
          # Si Python 3.13 n'est pas disponible, uv peut l'installer automatiquement
          # V√©rifier si Python 3.13 est requis et disponible via uv
          if ! python3 -c "import sys; assert sys.version_info >= (3, 13)" 2>/dev/null; then
            echo "‚ö†Ô∏è Python 3.13 requis mais version $PYTHON_VERSION d√©tect√©e"
            echo "‚ÑπÔ∏è uv installera automatiquement Python 3.13 lors de la cr√©ation du venv"
          fi
          
          # V√©rifier les versions install√©es
          node --version
          npm --version
          python --version || python3 --version
          uv --version

      - name: Checkout du code
        uses: actions/checkout@v6

      - name: Cache des d√©pendances Python avec uv
        uses: actions/cache@v5
        with:
          path: |
            ~/.cache/uv
            .venv
          key: ${{ runner.os }}-uv-${{ hashFiles('uv.lock', 'pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-uv-

      - name: Cr√©ation de l'environnement virtuel et installation des d√©pendances avec uv
        run: |
          export PATH="$HOME/.local/bin:$PATH"
          
          # Installer Python 3.13 via uv si n√©cessaire
          echo "üêç V√©rification/installation de Python 3.13 via uv..."
          uv python install 3.13 || echo "‚ö†Ô∏è uv python install a √©chou√©, utilisation de Python syst√®me"
          
          echo "üîß Cr√©ation de l'environnement virtuel avec uv (Python 3.13)..."
          uv venv --python 3.13 || uv venv || {
            echo "‚ö†Ô∏è √âchec de cr√©ation du venv avec Python 3.13, tentative avec Python syst√®me"
            uv venv
          }
          
          echo "üì¶ Installation des d√©pendances avec uv sync (depuis uv.lock)..."
          uv sync --dev
          
          echo "‚úÖ Environnement virtuel cr√©√© et d√©pendances install√©es avec succ√®s"
          source .venv/bin/activate
          python --version
          uv --version

      - name: Configuration de l'environnement de test
        run: |
          export PATH="$HOME/.local/bin:$PATH"
          source .venv/bin/activate
          # Cr√©er une cl√© de chiffrement de test si elle n'existe pas
          export ENCRYPTION_KEY=${ENCRYPTION_KEY:-$(python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())")}
          echo "ENCRYPTION_KEY=${ENCRYPTION_KEY}" >> $GITEA_ENV
          export FLASK_ENV=testing
          echo "FLASK_ENV=testing" >> $GITEA_ENV

      - name: Ex√©cution des tests unitaires
        env:
          ENCRYPTION_KEY: ${{ env.ENCRYPTION_KEY }}
          FLASK_ENV: testing
        run: |
          export PATH="$HOME/.local/bin:$PATH"
          echo "üß™ Ex√©cution des tests..."
          if [ ! -f ".venv/bin/activate" ]; then
            echo "‚ùå Le venv n'existe pas, cr√©ation avec uv..."
            uv venv
            uv sync --dev
          fi
          source .venv/bin/activate
          # V√©rifier si des tests existent
          if [ -d "tests" ] || find . -name "test_*.py" -o -name "*_test.py" 2>/dev/null | grep -q .; then
            echo "‚úÖ Tests trouv√©s, ex√©cution..."
            # Ex√©cuter pytest avec rapport de couverture
            pytest --cov=app --cov-report=term-missing --cov-report=xml --cov-report=html -v || exit 1
            echo "‚úÖ Tous les tests sont pass√©s"
          else
            echo "‚ö†Ô∏è Aucun test trouv√© dans le projet"
            echo "‚ÑπÔ∏è Le job passe mais pr√©voir d'ajouter des tests dans un r√©pertoire 'tests/' ou avec le pr√©fixe 'test_'"
            echo "‚ÑπÔ∏è Cr√©ez des fichiers comme tests/test_models.py, tests/test_routes.py, etc."
            # Ne pas faire √©chouer le pipeline si aucun test n'existe encore
            exit 0
          fi

      - name: Rapport de couverture (si disponible)
        if: success() || failure()
        run: |
          # Ne s'ex√©cuter que si le code a √©t√© checkout√© (v√©rifier la pr√©sence de pyproject.toml)
          if [ ! -f "pyproject.toml" ]; then
            echo "‚ö†Ô∏è Code non checkout√©, impossible de g√©n√©rer le rapport de couverture"
            exit 0
          fi
          
          export PATH="$HOME/.local/bin:$PATH"
          # V√©rifier que le venv existe avant de l'activer
          if [ -f ".venv/bin/activate" ]; then
            source .venv/bin/activate
          else
            echo "‚ö†Ô∏è Environnement virtuel non trouv√©, cr√©ation d'un venv temporaire..."
            uv venv .venv-temp 2>/dev/null || python -m venv .venv-temp 2>/dev/null || true
            if [ -f ".venv-temp/bin/activate" ]; then
              source .venv-temp/bin/activate
            fi
          fi
          
          if [ -f coverage.xml ]; then
            echo "üìä Rapport de couverture g√©n√©r√©"
            python -c "import xml.etree.ElementTree as ET; tree = ET.parse('coverage.xml'); root = tree.getroot(); line_rate = float(root.get('line-rate', 0)) * 100; print(f'Couverture de code: {line_rate:.2f}%')" || echo "Impossible de lire le rapport de couverture"
          else
            echo "‚ÑπÔ∏è Aucun rapport de couverture disponible"
          fi

  build-and-push:
    needs: test
    runs-on: docker
    steps:
      - name: Configuration du runner
        run: |
          apk add --no-cache \
          docker \
          docker-cli-buildx \
          git \
          curl \
          bash \
          nodejs \
          npm
          
          # V√©rifier la version de Node.js install√©e
          node --version
          npm --version
          
      - name: Checkout du code avec actions/checkout
        uses: actions/checkout@v6
        with:
          fetch-depth: 0

      # Calcul du hash des fichiers de d√©pendances pour le cache
      - name: Calcul du hash des fichiers de build
        id: cache-hash
        run: |
          # Calculer le hash des fichiers qui affectent le build Docker
          if command -v sha256sum >/dev/null 2>&1; then
            DOCKERFILE_HASH=$(sha256sum Dockerfile | cut -d' ' -f1)
            PYPROJECT_HASH=$(sha256sum pyproject.toml | cut -d' ' -f1)
          elif command -v sha256 >/dev/null 2>&1; then
            DOCKERFILE_HASH=$(sha256 -q Dockerfile)
            PYPROJECT_HASH=$(sha256 -q pyproject.toml)
          else
            # Fallback: utiliser md5sum si disponible
            DOCKERFILE_HASH=$(md5sum Dockerfile | cut -d' ' -f1)
            PYPROJECT_HASH=$(md5sum pyproject.toml | cut -d' ' -f1)
          fi
          
          # Combiner les hashs pour cr√©er une cl√© unique
          CACHE_KEY_HASH=$(echo "${DOCKERFILE_HASH}-${PYPROJECT_HASH}" | sha256sum | cut -d' ' -f1 || echo "${DOCKERFILE_HASH}-${PYPROJECT_HASH}")
          
          echo "DOCKERFILE_HASH=${DOCKERFILE_HASH}" >> $GITEA_ENV
          echo "PYPROJECT_HASH=${PYPROJECT_HASH}" >> $GITEA_ENV
          echo "CACHE_KEY_HASH=${CACHE_KEY_HASH}" >> $GITEA_ENV
          echo "Hash Dockerfile: ${DOCKERFILE_HASH:0:12}..."
          echo "Hash pyproject.toml: ${PYPROJECT_HASH:0:12}..."
          echo "Cl√© de cache: ${CACHE_KEY_HASH:0:16}..."

      # Configuration du cache Docker
      - name: Configuration du cache Docker
        uses: actions/cache@v5
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-buildx-${{ env.CACHE_KEY_HASH }}
          restore-keys: |
            ${{ runner.os }}-buildx-

      # D√©tecter la version
      - name: D√©tecter la version
        id: version
        run: |
          VERSION=$(git describe --tags --abbrev=0 2>/dev/null || echo "")
          if [ -z "$VERSION" ]; then
            BRANCH=$(git branch --show-current)
            if [[ "$BRANCH" =~ ^v[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
              VERSION="$BRANCH"
            fi
          fi
          if [ -z "$VERSION" ]; then
            VERSION="v1.0.0-dev"
          fi
          echo "VERSION=$VERSION" >> $GITEA_ENV
          echo "Version d√©tect√©e: $VERSION"

      # Authentification au registre
      - name: Login au registre Docker
        run: |
          echo "${{ secrets.REGISTRY_PASSWORD }}" | docker login forge.apacher.eu -u "${{ secrets.REGISTRY_USERNAME }}" --password-stdin

      # Construction et push de l'image - VERSION OPTIMIS√âE
      - name: Build et push de l'image Docker
        run: |
          set -e
          
          # Nettoyage pr√©ventif des anciens conteneurs BuildKit
          echo "Nettoyage pr√©ventif des conteneurs BuildKit..."
          docker container ls -q --filter "name=buildx_buildkit" | xargs -r docker container stop || true
          docker container ls -aq --filter "name=buildx_buildkit" | xargs -r docker container rm || true
          
          # Gestion intelligente du builder
          BUILDER_NAME="chronotrak-builder"
          
          if docker buildx ls | grep -q "$BUILDER_NAME"; then
            echo "Utilisation du builder existant: $BUILDER_NAME"
            docker buildx use $BUILDER_NAME
          else
            echo "Cr√©ation d'un nouveau builder: $BUILDER_NAME"
            docker buildx create --name $BUILDER_NAME --use
          fi
          
          # Build avec gestion d'erreurs et cache optimis√©
          echo "D√©marrage du build de l'image..."
          docker buildx build --provenance=false --push \
            --cache-from=type=local,src=/tmp/.buildx-cache \
            --cache-from=type=registry,ref=forge.apacher.eu/aurelien-dazy/chronotrak:buildcache \
            --cache-to=type=local,dest=/tmp/.buildx-cache-new,mode=max \
            --cache-to=type=registry,ref=forge.apacher.eu/aurelien-dazy/chronotrak:buildcache,mode=max \
            --build-arg VERSION="${{ env.VERSION }}" \
            --tag forge.apacher.eu/aurelien-dazy/chronotrak:latest \
            --tag forge.apacher.eu/aurelien-dazy/chronotrak:${{ env.VERSION }} \
            --progress=plain \
            .
          
          echo "Build termin√© avec succ√®s"

      # Gestion du cache et nettoyage S√âCURIS√â
      - name: Gestion du cache et nettoyage s√©curis√©
        if: always()
        run: |
          # Gestion du cache
          if [ -d "/tmp/.buildx-cache-new" ]; then
            rm -rf /tmp/.buildx-cache
            mv /tmp/.buildx-cache-new /tmp/.buildx-cache
            echo "Cache mis √† jour"
          fi
          
          # Nettoyage CIBL√â des conteneurs BuildKit seulement
          echo "Nettoyage cibl√© des conteneurs BuildKit..."
          BUILDKIT_CONTAINERS=$(docker container ls -aq --filter "name=buildx_buildkit" --filter "label=org.mobyproject.buildkit.worker.executor=oci" 2>/dev/null || echo "")
          
          if [ ! -z "$BUILDKIT_CONTAINERS" ]; then
            echo "Suppression des conteneurs BuildKit: $BUILDKIT_CONTAINERS"
            docker container rm -f $BUILDKIT_CONTAINERS 2>/dev/null || true
          else
            echo "Aucun conteneur BuildKit √† supprimer"
          fi
          
          # Nettoyage du cache BuildKit uniquement (pas docker system)
          echo "Nettoyage du cache BuildKit..."
          docker buildx prune -f --reserved-space=1GB 2>/dev/null || true
          
          # Nettoyage des images build temporaires sans tag (plus s√ªr)
          echo "Nettoyage des images temporaires..."
          docker image prune -f --filter "dangling=true" 2>/dev/null || true
          
          # Nettoyage des volumes BuildKit uniquement (S√âCURIS√â)
          echo "Nettoyage des volumes BuildKit..."
          docker volume ls --filter "name=buildx_buildkit_" --format "{{.Name}}" | xargs -r docker volume rm 2>/dev/null || true
          
          # Affichage des statistiques pour monitoring
          echo "=== √âtat final des builders ==="
          docker buildx ls
          echo "=== Conteneurs BuildKit restants ==="
          docker container ls --filter "name=buildx_buildkit" --format "table {{.Names}}\t{{.Status}}\t{{.CreatedAt}}" || true
          echo "=== Utilisation disque Docker ==="
          docker system df || true

      # Analyse de s√©curit√© avec Trivy (uniquement sur main/master)
      - name: Installation et ex√©cution de Trivy
        if: gitea.ref == 'refs/heads/main' || gitea.ref == 'refs/heads/master'
        run: |
          apk add --no-cache curl
          curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin
          # Ignorer les vuln√©rabilit√©s d√©j√† corrig√©es (status: fixed)
          trivy image forge.apacher.eu/aurelien-dazy/chronotrak:latest \
            --severity HIGH,CRITICAL \
            --ignore-status fixed \
            --exit-code 1

  deploy:
    needs: build-and-push
    runs-on: docker
    container:
      image: docker:latest
    steps:
      # Installation des d√©pendances SSH
      - name: Installation des d√©pendances SSH
        run: |
          apk add --no-cache openssh-client bash file
          mkdir -p ~/.ssh
          echo "StrictHostKeyChecking=no" > ~/.ssh/config
          chmod 600 ~/.ssh/config

      # Configuration de la cl√© SSH
      - name: Configuration de la cl√© SSH
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.PROD_SSH_KEY }}" > ~/.ssh/id_ed25519
          chmod 600 ~/.ssh/id_ed25519

      # Test de connexion SSH
      - name: Test de connexion SSH
        run: |
          ssh -o StrictHostKeyChecking=no -i ~/.ssh/id_ed25519 -p ${{ secrets.PROD_SSH_PORT }} ${{ secrets.PROD_SSH_USERNAME }}@${{ secrets.PROD_SSH_HOST }} "echo 'Connexion SSH r√©ussie'"

      # Sauvegarde de la base de donn√©es (uniquement sur main/master)
      - name: Sauvegarde de la base de donn√©es
        if: gitea.ref == 'refs/heads/main' || gitea.ref == 'refs/heads/master'
        run: |
          ssh -o StrictHostKeyChecking=no -i ~/.ssh/id_ed25519 -p ${{ secrets.PROD_SSH_PORT }} ${{ secrets.PROD_SSH_USERNAME }}@${{ secrets.PROD_SSH_HOST }} "
            set -ex
            echo 'Cr√©ation du r√©pertoire de sauvegarde'
            mkdir -p ${{ secrets.PROJECT_DIRECTORY }}/backups
            
            echo 'Sauvegarde de la base de donn√©es'
            cd ${{ secrets.PROJECT_DIRECTORY }}
            
            BACKUP_DATE=\$(date +'%Y%m%d_%H%M%S')
            
            docker run --rm -v docker-chronotrak_chronotrak_data:/app/instance -v ${{ secrets.PROJECT_DIRECTORY }}/backups:/backup alpine sh -c \"
              if [ -f /app/instance/chronotrak.db ]; then
                cp /app/instance/chronotrak.db /backup/chronotrak_\${BACKUP_DATE}.db
                echo 'Sauvegarde effectu√©e avec succ√®s'
              else
                echo 'Base de donn√©es non trouv√©e dans le volume'
                touch /backup/chronotrak_\${BACKUP_DATE}_empty.txt
              fi
            \"
            
            cd ${{ secrets.PROJECT_DIRECTORY }}/backups
            ls -t chronotrak_*.db | tail -n +11 | xargs -r rm
            
            echo 'Sauvegarde termin√©e'
          "
      
      # D√©ploiement sur le serveur de production
      - name: D√©ploiement sur le serveur de production
        run: |
          ssh -o StrictHostKeyChecking=no -i ~/.ssh/id_ed25519 -p ${{ secrets.PROD_SSH_PORT }} ${{ secrets.PROD_SSH_USERNAME }}@${{ secrets.PROD_SSH_HOST }} "
            set -ex
            echo 'D√©but du d√©ploiement'
            cd ${{ secrets.PROJECT_DIRECTORY }}
            
            # Login au registre
            docker login forge.apacher.eu -u ${{ secrets.REGISTRY_USERNAME }} -p ${{ secrets.REGISTRY_PASSWORD }}
            
            # Pull et d√©ploiement
            ENVIRONMENT=production docker compose pull
            ENVIRONMENT=production docker compose up -d
            
            # Nettoyage post-d√©ploiement
            docker system prune -f
            # Nettoyage des volumes BuildKit uniquement sur le serveur de production
            docker volume ls --filter "name=buildx_buildkit_" --format "{{.Name}}" | xargs -r docker volume rm 2>/dev/null || true
            docker logout forge.apacher.eu
            
            echo 'D√©ploiement termin√© avec succ√®s'
          "